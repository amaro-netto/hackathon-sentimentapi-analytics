{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:04.151746100Z",
     "start_time": "2026-01-13T01:59:49.672044500Z"
    }
   },
   "source": [
    "# Install dependencies\n",
    "import kagglehub\n",
    "\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import joblib"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s050741159\\Desktop\\hackathon-sentimentapi-analytics\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:09.776780400Z",
     "start_time": "2026-01-13T02:00:04.157140500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = 'olist_order_reviews_dataset.csv'\n",
    "\n",
    "# Load the latest version\n",
    "df_olist = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"olistbr/brazilian-ecommerce\", file_path, pandas_kwargs={\"encoding\":'utf-8', \"quotechar\":'\"', \"on_bad_lines\":'skip'})\n",
    "\n",
    "hf = load_dataset(\"ruanchaves/b2w-reviews01\")\n",
    "\n",
    "df_b2w = pd.concat([hf['train'].to_pandas()], ignore_index=True)"
   ],
   "id": "864f3254cac57e64",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:10.375563500Z",
     "start_time": "2026-01-13T02:00:10.196083600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_b2w.drop(axis=\"columns\", columns=(['submission_date', 'reviewer_id', 'product_id', 'product_name',\n",
    "       'product_brand', 'site_category_lv1', 'site_category_lv2',\n",
    "       'review_title', 'reviewer_birth_year', 'reviewer_gender', 'reviewer_state', 'recommend_to_a_friend']), inplace=True)"
   ],
   "id": "26e71b575ac25127",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:10.446753100Z",
     "start_time": "2026-01-13T02:00:10.392443300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_olist.drop(axis=\"columns\", columns=(['review_id', 'order_id','review_creation_date',\n",
    "       'review_answer_timestamp', 'review_comment_title']), inplace=True)"
   ],
   "id": "91c165d695075f68",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:10.487168600Z",
     "start_time": "2026-01-13T02:00:10.447752100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_b2w.dropna(subset=['review_text'], inplace=True)\n",
    "\n",
    "df_olist.dropna(subset=['review_comment_message'], inplace=True)"
   ],
   "id": "3f1b04e9284ea72",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:10.526305600Z",
     "start_time": "2026-01-13T02:00:10.489369400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapa_sentimentos = {\n",
    "    1: 'Negativo',\n",
    "    2: 'Negativo',\n",
    "    3: 'Neutro',\n",
    "    4: 'Positivo',\n",
    "    5: 'Positivo'\n",
    "}\n",
    "\n",
    "df_b2w['sentimento'] = df_b2w['overall_rating'].map(mapa_sentimentos)\n",
    "df_olist['sentimento'] = df_olist['review_score'].map(mapa_sentimentos)\n",
    "\n",
    "df_olist.drop(axis=\"columns\", columns=(['review_score']), inplace=True)\n",
    "\n",
    "df_b2w.drop(axis=\"columns\", columns=(['overall_rating']), inplace=True)"
   ],
   "id": "7dbd535dd9aac496",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:10.543674400Z",
     "start_time": "2026-01-13T02:00:10.529303400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_olist.columns = ['texto', 'sentimento']\n",
    "df_b2w.columns = ['texto', 'sentimento']"
   ],
   "id": "5852df1ae74a178",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:00:58.168505400Z",
     "start_time": "2026-01-13T02:00:58.120329400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agora, concatenamos os dois datasets, ao mesmo tempo em que resetamos os index e os \"embaralhamos\"\n",
    "\n",
    "# 1. Concatenar os dois dataframes (empilhar um sobre o outro)\n",
    "# ignore_index=True ajuda a não duplicar índices antigos, mas o reset abaixo garante a limpeza final\n",
    "df_pt = pd.concat([df_olist, df_b2w], ignore_index=True)\n",
    "\n",
    "# 2. Embaralhar as linhas\n",
    "# frac=1 significa \"retorne 100% dos dados\", mas em ordem aleatória\n",
    "# reset_index(drop=True) recria o índice de 0 a N, para não ficar tudo bagunçado (ex: 5, 100, 2...)\n",
    "df_pt = df_final.sample(frac=1, random_state=42).reset_index(drop=True)"
   ],
   "id": "3292840af820c266",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:01:04.913960Z",
     "start_time": "2026-01-13T02:01:04.782713300Z"
    }
   },
   "cell_type": "code",
   "source": "df_pt.info()",
   "id": "1aa70df248a5b14a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 170075 entries, 0 to 170074\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   texto       170075 non-null  object\n",
      " 1   sentimento  170075 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:01:09.273910500Z",
     "start_time": "2026-01-13T02:01:05.992510100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"es\")\n",
    "\n",
    "df_es = pd.concat([dataset['train'].to_pandas(), dataset['test'].to_pandas(), dataset['validation'].to_pandas()], ignore_index=True)"
   ],
   "id": "3738312eddc3aa9f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:01:18.668533600Z",
     "start_time": "2026-01-13T02:01:18.631262900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DF_ES:\n",
    "# 0,1 => Negativo - 2 => Neutro - 3,4 => Positivo. Criamos uma nova coluna com esse \"Mapeamento\" usando a função .map()\n",
    "\n",
    "mapa_sentimentos = {\n",
    "    0: 'Negativo',\n",
    "    1: 'Negativo',\n",
    "    2: 'Neutro',\n",
    "    3: 'Positivo',\n",
    "    4: 'Positivo'\n",
    "}\n",
    "\n",
    "df_es['sentimento'] = df_es['label'].map(mapa_sentimentos)"
   ],
   "id": "af7a24d9f7efef73",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:01:35.148824800Z",
     "start_time": "2026-01-13T02:01:35.114431500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_es.drop(axis=\"columns\", columns=['id', 'label', 'label_text'], inplace=True)\n",
    "df_es.columns = ['texto', 'sentimento']"
   ],
   "id": "5215bf579cf523df",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:02:08.857932900Z",
     "start_time": "2026-01-13T02:02:08.828256500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_pt['idioma']=\"pt\"\n",
    "df_es['idioma']='es'"
   ],
   "id": "266b44b6d8cc9e7d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:02:30.048142900Z",
     "start_time": "2026-01-13T02:02:29.919771600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Concatena\n",
    "df_multi = pd.concat([df_pt, df_es])\n",
    "\n",
    "# 2. Embaralha e reseta o índice\n",
    "# frac=1 -> Pega 100% das linhas aleatoriamente\n",
    "# random_state=42 -> Garante que o embaralhamento seja sempre o mesmo (importante para reproduzir seus testes)\n",
    "# reset_index(drop=True) -> Cria um índice novo (0, 1, 2...) e joga fora o antigo bagunçado\n",
    "\n",
    "df_multi = df_multi.sample(frac=1, random_state=42).reset_index(drop=True)"
   ],
   "id": "76fb12a06b77b6be",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:02:42.135147100Z",
     "start_time": "2026-01-13T02:02:41.108692100Z"
    }
   },
   "cell_type": "code",
   "source": "df_multi.to_csv('../datasets/df_multi_concatenado.csv', encoding='utf-8',index=False, header=True, sep=';')",
   "id": "d1f97d0266e1d4ca",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T02:03:34.779234Z",
     "start_time": "2026-01-13T02:02:57.346657300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# X: textos; y: DataFrame com duas colunas\n",
    "X = df_multi['texto']\n",
    "y = df_multi[['sentimento', 'idioma']]\n",
    "\n",
    "base_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "modelo_multi = make_pipeline(\n",
    "    TfidfVectorizer(max_features=10000,\n",
    "                    strip_accents='unicode',\n",
    "                    lowercase=True,\n",
    "                    ngram_range=(1, 2)),\n",
    "    MultiOutputClassifier(base_clf)\n",
    ")\n",
    "modelo_multi.fit(X, y)\n",
    "\n",
    "print(\"⏳ Treinando modelo...\")\n",
    "\n",
    "# 3. Salvar\n",
    "joblib.dump(modelo_multi, \"../models/modelo_multi.joblib\")\n",
    "print(\"✅ Novo modelo MULTI salvo em: modelo_multi.joblib\")"
   ],
   "id": "f58eeaefc578c21a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Treinando modelo...\n",
      "✅ Novo modelo MULTI salvo em: modelo_multi.joblib\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "84727b3cd8126055"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
